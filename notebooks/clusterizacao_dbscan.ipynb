{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack, vstack\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio_destino = '../vetores'\n",
    "\n",
    "tfidf_title = joblib.load(f\"{diretorio_destino}/tfidf_title.pkl\")\n",
    "tfidf_abstract = joblib.load(f\"{diretorio_destino}/tfidf_abstract.pkl\")\n",
    "tfidf_combined = hstack([tfidf_title, tfidf_abstract])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.5\n",
    "min_samples = 10\n",
    "batch_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando batch 1: 20000 itens.\n",
      "Processando batch 2: 20000 itens.\n",
      "Processando batch 3: 20000 itens.\n",
      "Processando batch 4: 20000 itens.\n",
      "Processando batch 5: 20000 itens.\n",
      "Processando batch 6: 20000 itens.\n",
      "Processando batch 7: 20000 itens.\n",
      "Processando batch 8: 20000 itens.\n",
      "Processando batch 9: 20000 itens.\n",
      "Processando batch 10: 20000 itens.\n",
      "Processando batch 11: 20000 itens.\n",
      "Processando batch 12: 20000 itens.\n",
      "Processando batch 13: 20000 itens.\n",
      "Processando batch 14: 20000 itens.\n",
      "Processando batch 15: 20000 itens.\n",
      "Processando batch 16: 20000 itens.\n",
      "Processando batch 17: 20000 itens.\n",
      "Processando batch 18: 20000 itens.\n",
      "Processando batch 19: 20000 itens.\n",
      "Processando batch 20: 20000 itens.\n",
      "Processando batch 21: 20000 itens.\n",
      "Processando batch 22: 20000 itens.\n",
      "Processando batch 23: 20000 itens.\n",
      "Processando batch 24: 20000 itens.\n",
      "Processando batch 25: 20000 itens.\n",
      "Processando batch 26: 20000 itens.\n",
      "Processando batch 27: 20000 itens.\n",
      "Processando batch 28: 20000 itens.\n",
      "Processando batch 29: 20000 itens.\n",
      "Processando batch 30: 20000 itens.\n",
      "Processando batch 31: 20000 itens.\n",
      "Processando batch 32: 20000 itens.\n",
      "Processando batch 33: 20000 itens.\n",
      "Processando batch 34: 20000 itens.\n",
      "Processando batch 35: 20000 itens.\n",
      "Processando batch 36: 20000 itens.\n",
      "Processando batch 37: 20000 itens.\n",
      "Processando batch 38: 20000 itens.\n",
      "Processando batch 39: 20000 itens.\n",
      "Processando batch 40: 20000 itens.\n",
      "Processando batch 41: 20000 itens.\n",
      "Processando batch 42: 7533 itens.\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "\n",
    "for i in range(0, tfidf_combined.shape[0], batch_size):\n",
    "    batch = tfidf_combined[i:i+batch_size]\n",
    "    print(f\"Processando batch {i // batch_size + 1}: {batch.shape[0]} itens.\")\n",
    "    batch_labels = dbscan.fit_predict(batch)\n",
    "    all_labels.extend(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusterização concluída. Total de clusters encontrados: 6\n"
     ]
    }
   ],
   "source": [
    "all_labels = np.array(all_labels)\n",
    "print(f\"Clusterização concluída. Total de clusters encontrados: {len(set(all_labels)) - (1 if -1 in all_labels else 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "X_sample, labels_sample = resample(tfidf_combined, all_labels, n_samples=n_samples, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score (subamostra): -0.00023132931628465044\n"
     ]
    }
   ],
   "source": [
    "if len(set(labels_sample)) > 1:\n",
    "    silhouette_avg = silhouette_score(X_sample, labels_sample)\n",
    "    print(f\"Silhouette Score (subamostra): {silhouette_avg}\")\n",
    "else:\n",
    "    print(\"DBSCAN não formou clusters significativos na subamostra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2, random_state=42)\n",
    "# X_reduced = pca.fit_transform(X_sample.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# unique_labels = set(labels_sample)\n",
    "# for label in unique_labels:\n",
    "#     mask = labels_sample == label\n",
    "#     plt.scatter(X_reduced[mask, 0], X_reduced[mask, 1], label=f'Cluster {label}' if label != -1 else \"Ruído\", s=10)\n",
    "# plt.title(\"Dispersão com PCA (DBSCAN)\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(\"../data/dblp-v10-processado.parquet\")\n",
    "# df['cluster_dbscan'] = all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cluster in unique_labels:\n",
    "#     if cluster == -1:  # Ignorar ruído\n",
    "#         continue\n",
    "#     cluster_text = df[df['cluster_dbscan'] == cluster]['title'].apply(lambda x: ' '.join(x)).str.cat(sep=' ')\n",
    "#     wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cluster_text)\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f'Nuvem de Palavras - Cluster {cluster}')\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
